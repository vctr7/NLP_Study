# Sentence Embeddings Using Korean Corpora(한국어 임베딩) Study

<img src="https://i.imgur.com/j03ENCc.jpg" width="320" height="400"> [Official Website](https://ratsgo.github.io/)

During NAVER corp. Winter Internship(Jan. 2020 ~ Feb. 2020)


---

## Index

### 01. 서론

- 1.1 임베딩이란

- 1.2 임베딩의 역할

- 1.3 임베딩 기법의 역사와 종류

### 02. 벡터가 어떻게 의미를 가지게 되는가

- 2.1 자연어 계산과 이해

- 2.2 [How does a vector have meaning](https://github.com/vctr7/NLP_Study/blob/master/ppt/How%20does%20a%20vector%20have%20meaning.pdf)

- 2.3 단어가 어떤 순서로 쓰였는가

- 2.4 어떤 단어가 같이 쓰였는가

### 03. 한국어 전처리

- 3.1 [Data Pre-processing](https://github.com/vctr7/NLP_Study/blob/master/ppt/Data%20Pre-processing.pdf)

- 3.2 지도 학습 기반 형태소 분석

- 3.3 비지도 학습 기반 형태소 분석

### 04. Word Embeddings

- 4.1 [NPLM](https://github.com/vctr7/NLP_Study/blob/master/ppt/NPLM.pdf)

- 4.2 [Word2Vec](https://github.com/vctr7/NLP_Study/blob/master/ppt/Word2Vec.pdf)

- 4.3 [FastText](https://github.com/vctr7/NLP_Study/blob/master/ppt/FastText.pdf)

- 4.4 [Latent Semantic Analysis](https://github.com/vctr7/NLP_Study/blob/master/ppt/LSA.pdf)

- 4.5 [GloVe](https://github.com/vctr7/NLP_Study/blob/master/ppt/GloVE.pdf)

-	4.6 [Swivel](https://github.com/vctr7/NLP_Study/blob/master/ppt/Swivel.pdf)

- 4.7 어떤 단어 임베딩을 사용할 것인가

- 4.8 가중 임베딩

- [Model Comparison](https://github.com/vctr7/NLP_Study/blob/master/ppt/Word%20Embedding%20Results.pdf)



### 05. Sentence Embeddings

- 5.1 LSA

- 5.2 Doc2Vec

- 5.3 Latent Dirichlet Allocation

- 5.4 ELMo

- 5.5 Transformer Network

-	5.6 BERT



### 06. 임베딩 파인 튜닝

- 6.1 Pre-train & Fine-tuning

-	6.2 분류를 위한 파이프라인 만들기

- 6.3 단어 임베딩 활용

- 6.4 ELMo 활용 

- 6.5 BERT 활용

- 6.6 어떤 문장 임베딩을 사용할 것인가



### Appendix

- A Linear Algebra

- B Probability Theory

- C Neural Network

-	D Korean Linguistics



## References

- [https://ratsgo.github.io/](https://ratsgo.github.io/)
- [https://github.com/ratsgo/embedding](https://github.com/ratsgo/embedding)




 I do not own any rights.

Copyright  © acorn publishing Co., 2019. All rights reserved. 
